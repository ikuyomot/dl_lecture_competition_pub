{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画像のエンコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "model.eval()\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def load_image_paths(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        image_paths = f.read().splitlines()\n",
    "    return image_paths\n",
    "\n",
    "def correct_paths(image_paths):\n",
    "    corrected_paths = []\n",
    "    for path in image_paths:\n",
    "        if '/' not in path:\n",
    "            parts = path.split('_')\n",
    "            if len(parts) > 1:\n",
    "                parent_dir = '_'.join(parts[:-1]) \n",
    "                path = os.path.join(parent_dir, path)\n",
    "        corrected_paths.append(path)\n",
    "    return corrected_paths\n",
    "\n",
    "def process_images_and_texts(image_paths):\n",
    "    corrected_paths = correct_paths(image_paths)\n",
    "    texts = [path.replace(\"\\\\\", \"/\").split('/')[0] for path in corrected_paths]\n",
    "    images = [Image.open(os.path.join('images\\\\Images', path.replace(\"/\", \"\\\\\"))).convert(\"RGB\") for path in corrected_paths]\n",
    "    return images, texts\n",
    "\n",
    "def encode_images(images, texts=None):\n",
    "    inputs = processor(images=images, text=texts, return_tensors=\"pt\", padding=True).to(device) if texts else processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).image_embeds if texts else model.get_image_features(**inputs)\n",
    "    return outputs.cpu()\n",
    "\n",
    "def save_encoded_features(image_paths, batch_size, save_path, include_texts=False):\n",
    "    encoded_features = []\n",
    "    num_batches = len(image_paths) // batch_size + int(len(image_paths) % batch_size != 0)\n",
    "\n",
    "    for i in tqdm(range(num_batches)):\n",
    "        batch_paths = image_paths[i*batch_size:(i+1)*batch_size]\n",
    "        images, texts = process_images_and_texts(batch_paths)\n",
    "        outputs = encode_images(images, texts if include_texts else None)\n",
    "        encoded_features.append(outputs)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    encoded_features = torch.cat(encoded_features)\n",
    "    print(f\"Size: {encoded_features.shape}\")\n",
    "    torch.save(encoded_features, save_path)\n",
    "\n",
    "# Train set encoding\n",
    "train_image_paths = load_image_paths('data\\\\train_image_paths.txt')\n",
    "save_encoded_features(train_image_paths, batch_size=512, save_path='data\\\\train_image_text_encoded.pt', include_texts=True)\n",
    "\n",
    "# Validation set encoding\n",
    "val_image_paths = load_image_paths('data\\\\val_image_paths.txt')\n",
    "save_encoded_features(val_image_paths, batch_size=512, save_path='data\\\\val_image_encoded.pt', include_texts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 事前学習\n",
    "\n",
    "s, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy\n",
    "from omegaconf import DictConfig\n",
    "from termcolor import cprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torchinfo import summary\n",
    "from einops import rearrange\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SubjectLayers(nn.Module):\n",
    "    \"\"\"Per subject linear layer.\n",
    "    The code is modified from\n",
    "    https://github.com/facebookresearch/brainmagick/blob/1aa77d2dbd801b4901aeadfd8606d26a89ee7e3e/bm/models/common.py#L45\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, n_subjects: int, init_id: bool = False):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(n_subjects, in_channels, out_channels))\n",
    "        if init_id:\n",
    "            assert in_channels == out_channels\n",
    "            self.weights.data[:] = torch.eye(in_channels)[None]\n",
    "        self.weights.data *= 1 / in_channels**0.5\n",
    "\n",
    "    def forward(self, x, subjects):\n",
    "        _, C, D = self.weights.shape\n",
    "        weights = self.weights.gather(0, subjects.view(-1, 1, 1).expand(-1, C, D))\n",
    "        # print(x.shape, weights.shape)\n",
    "        return torch.einsum(\"bct,bcd->bdt\", x, weights)\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    \"\"\"\n",
    "        The code is modified from https://github.com/eeyhsong/NICE-EEG\n",
    "    \"\"\"\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "class ProjMeg(nn.Sequential):\n",
    "    \"\"\"\n",
    "        The code is modified from https://github.com/eeyhsong/NICE-EEG\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim=512, out_dim=512, drop_proj=0.5):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_dim, out_dim),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(out_dim, out_dim),\n",
    "                nn.Dropout(drop_proj),\n",
    "            )),\n",
    "            nn.LayerNorm(out_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "class ProjImg(nn.Sequential):\n",
    "    \"\"\"\n",
    "        The code is modified from https://github.com/eeyhsong/NICE-EEG\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim=512, out_dim=512, drop_proj=0.3):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_dim, out_dim),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(out_dim, out_dim),\n",
    "                nn.Dropout(drop_proj),\n",
    "            )),\n",
    "            nn.LayerNorm(out_dim),\n",
    "        )\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        The code is modified from https://github.com/eeyhsong/NICE-EEG\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_num=250, inter=30, num_channel=271):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.sequence_num = sequence_num\n",
    "        self.inter = inter\n",
    "        self.extract_sequence = int(self.sequence_num / self.inter) \n",
    "        self.query = nn.Sequential(\n",
    "            nn.Linear(num_channel, num_channel),\n",
    "            nn.LayerNorm(num_channel),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.key = nn.Sequential(\n",
    "            nn.Linear(num_channel, num_channel),\n",
    "            nn.LayerNorm(num_channel),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(num_channel, num_channel),\n",
    "            nn.LayerNorm(num_channel),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "        self.drop_out = nn.Dropout(0)\n",
    "        self.pooling = nn.AvgPool2d(kernel_size=(1, self.inter), stride=(1, self.inter))\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        temp = rearrange(x, 'b o c s -> b o s c')\n",
    "        # print(temp.shape)\n",
    "        query = self.query(temp)\n",
    "        key = self.key(temp)\n",
    "        # print(query.shape, key.shape)\n",
    "\n",
    "        scaling = self.extract_sequence ** 0.5\n",
    "\n",
    "        attn_scores = torch.einsum('b o s c, b o s m -> b o c m', query, key) / scaling\n",
    "        attn_scores = F.softmax(attn_scores, dim=-1)\n",
    "        attn_scores = self.drop_out(attn_scores)\n",
    "        # print(attn_scores.shape)\n",
    "        out = torch.einsum('b o c s, b o c m -> b o c s', x, attn_scores)\n",
    "        # print(out.shape)\n",
    "        out = rearrange(out, 'b o c s -> b o s c')\n",
    "        out = self.projection(out)\n",
    "        out = rearrange(out, 'b o s c -> b o c s')\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class EEG_GAT(nn.Module):\n",
    "    \"\"\"\n",
    "        The code is modified from https://github.com/eeyhsong/NICE-EEG\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=281, out_channels=281):\n",
    "        super(EEG_GAT, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.conv1 = GATConv(in_channels=in_channels, out_channels=out_channels, heads=1)\n",
    "\n",
    "        self.num_channels = 271\n",
    "        # Create a list of tuples representing all possible edges between channels\n",
    "        self.edge_index_list = torch.Tensor([(i, j) for i in range(self.num_channels) for j in range(self.num_channels) if i != j]).to(device)\n",
    "        # Convert the list of tuples to a tensor\n",
    "        self.edge_index = torch.tensor(self.edge_index_list, dtype=torch.long).t().contiguous().to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, _, num_channels, num_features = x.size()\n",
    "        \n",
    "        # Reshape x for GATConv\n",
    "        x = x.squeeze(1)  # [10, 1, 271, 281] -> [10, 271, 281]\n",
    "        # print(x.shape)\n",
    "        # x = x.permute(0, 2, 1)  # [10, 271, 281] -> [10, 281, 271]\n",
    "        # print(x.shape)\n",
    "        x = x.reshape(batch_size * num_channels, num_features)  # [10, 271, 281] -> [2710, 281]\n",
    "        # print(x.shape)\n",
    "        # Apply GATConv\n",
    "        out = self.conv1(x, self.edge_index)  # [2810, 281] -> [2810, 281]\n",
    "        # print(out.shape)\n",
    "        # Reshape the output back to the original form\n",
    "        out = out.view(batch_size, num_channels, num_features)  # [2810, 271] -> [10, 281, 271]\n",
    "        # print(out.shape)\n",
    "        # out = out.permute(0, 2, 1)  # [10, 281, 271] -> [10, 271, 281]\n",
    "        out = out.unsqueeze(1)  # [10, 271, 281] -> [10, 1, 271, 281]\n",
    "        # print(out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MEGEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The code is modified from https://github.com/ChiShengChen/MUSE_EEG\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channel=271, len_sec=281, emb_dim=40, out_dim=512, m1=25, m2=51, s=5, n_subjects=4):\n",
    "        super(MEGEncoder, self).__init__()\n",
    "        self.sa = ResidualAdd(nn.Sequential(\n",
    "            EEG_GAT(),\n",
    "            nn.Dropout(0.3)\n",
    "        ))\n",
    "        # self.temporal_conv = nn.Conv2d(1, emb_dim, kernel_size=(1, m1), stride=(1, 1))\n",
    "        # self.pool = nn.AvgPool2d(kernel_size=(1, m2), stride=(1, s))\n",
    "        # self.spatial_conv = nn.Conv2d(emb_dim, emb_dim, kernel_size=(n_channel, 1), stride=(1, 1))\n",
    "        # self.bn1 = nn.BatchNorm2d(emb_dim)\n",
    "        # self.bn2 = nn.BatchNorm2d(emb_dim)\n",
    "        self.attn_1 = nn.MultiheadAttention(embed_dim=emb_dim, num_heads=4)\n",
    "        self.attn_2 = nn.MultiheadAttention(embed_dim=emb_dim, num_heads=4)\n",
    "        self.attn_3 = nn.MultiheadAttention(embed_dim=emb_dim, num_heads=8, dropout=0.75)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.subject_layers = SubjectLayers(in_channels=emb_dim, out_channels=emb_dim, n_subjects=n_subjects)\n",
    "\n",
    "        self.tsconv = nn.Sequential(\n",
    "            nn.Conv2d(1, emb_dim, (1, m1), (1, 1)),\n",
    "            nn.AvgPool2d((1, m2), (1, s)),\n",
    "            nn.BatchNorm2d(emb_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(emb_dim, emb_dim, (n_channel, 1), (1, 1)),\n",
    "            nn.BatchNorm2d(emb_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.stconv = nn.Sequential(\n",
    "            nn.Conv2d(1, emb_dim, (n_channel, 1), (1, 1)),\n",
    "            nn.BatchNorm2d(emb_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(emb_dim, emb_dim, (1, m1), (1, 1)),\n",
    "            nn.AvgPool2d((1, m2), (1, s)),\n",
    "            nn.BatchNorm2d(emb_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        fc_in_dim = 2 * emb_dim * ((len_sec - m1 - m2 + 1) // s + 1)\n",
    "        # print(fc_in_dim)\n",
    "        self.fc = nn.Linear(fc_in_dim, out_dim) \n",
    "        self.do = nn.Dropout(0.5)\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(emb_dim) \n",
    "        self.norm_2 = nn.LayerNorm(emb_dim)\n",
    "        self.norm_3 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "    def forward(self, x, subjects):\n",
    "        x = self.sa(x)\n",
    "        # print(x.shape)\n",
    "        # x = self.temporal_conv(x)\n",
    "        # # print(x.shape)\n",
    "        # x = self.pool(x)\n",
    "        # # print(x.shape)\n",
    "        # x = self.bn1(x)\n",
    "        # # print(x.shape)\n",
    "        # x = F.elu(x)\n",
    "        # # print(x.shape)\n",
    "        # x = self.spatial_conv(x)\n",
    "        # # print(x.shape)\n",
    "        # x = self.bn2(x)\n",
    "        # # print(x.shape)\n",
    "        # x = F.elu(x)\n",
    "        # # print(x.shape)\n",
    "        # x = self.do(x)\n",
    "        ts = self.tsconv(x).squeeze(2).permute(2, 0, 1)\n",
    "        # print(x.shape)\n",
    "        st = self.stconv(x).squeeze(2).permute(2, 0, 1)\n",
    "\n",
    "        attn_ts, _ = self.attn_1(ts, ts, ts)\n",
    "        attn_st, _ = self.attn_2(st, st, st)\n",
    "\n",
    "        bf_ts_features = self.norm_1(attn_ts + ts)\n",
    "        bf_st_features = self.norm_2(attn_st + st)\n",
    "\n",
    "        combined_features = torch.cat((bf_ts_features, bf_st_features), dim=0)\n",
    "        attn_combined, _ = self.attn_3(combined_features, combined_features, combined_features)\n",
    "        x = self.norm_3(attn_combined + combined_features).permute(1, 2, 0)\n",
    "\n",
    "        # Apply the subject layers\n",
    "        x = self.subject_layers(x, subjects)\n",
    "\n",
    "        # print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        # print(x.shape)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class ContrastiveModel(nn.Module):\n",
    "    \"\"\"\n",
    "        The code is modified from https://github.com/eeyhsong/NICE-EEG\n",
    "    \"\"\"\n",
    "    def __init__(self, meg_encoder, proj_meg, proj_img):\n",
    "        super(ContrastiveModel, self).__init__()\n",
    "        self.meg_encoder = meg_encoder\n",
    "        self.proj_meg = proj_meg\n",
    "        self.proj_img = proj_img\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    def forward(self, X, image_features, subjects):\n",
    "        meg_features = self.meg_encoder(X, subjects)\n",
    "        eeg_features = self.proj_meg(meg_features)\n",
    "        img_features = self.proj_img(image_features)\n",
    "\n",
    "        eeg_features = eeg_features / eeg_features.norm(dim=1, keepdim=True)\n",
    "        img_features = img_features / img_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        logits = self.logit_scale.exp() * torch.matmul(eeg_features, img_features.t())\n",
    "        return logits\n",
    "    \n",
    "# meg_encoder = MEGEncoder(out_dim=512, m1=19, m2=10, s=4).to(\"cpu\")\n",
    "# input_data = (torch.randn(10, 1, 271, 281), torch.randint(0, 4, (10,)))\n",
    "# model_summary = summary(meg_encoder, input_data=input_data)\n",
    "# model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThingsMEGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split: str, data_dir: str = \"data\", baseline_frames: int = 50) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        assert split in [\"train\", \"val\", \"test\"], f\"Invalid split: {split}\"\n",
    "        self.split = split\n",
    "        self.num_classes = 1854\n",
    "        self.baseline_frames = baseline_frames\n",
    "        \n",
    "        self.X = torch.load(os.path.join(data_dir, f\"{split}_X.pt\"))\n",
    "        self.subject_idxs = torch.load(os.path.join(data_dir, f\"{split}_subject_idxs.pt\"))\n",
    "        \n",
    "        if split in [\"train\", \"val\"]:\n",
    "            self.y = torch.load(os.path.join(data_dir, f\"{split}_y.pt\"))\n",
    "            self.img_features = torch.load(os.path.join(data_dir, f\"{split}_image_encoded.pt\"))\n",
    "            assert len(torch.unique(self.y)) == self.num_classes, \"Number of classes do not match.\"\n",
    "\n",
    "        self.apply_baseline_correction()\n",
    "\n",
    "    def apply_baseline_correction(self):\n",
    "        baseline = self.X[:, :, :self.baseline_frames].mean(dim=2, keepdim=True)\n",
    "        self.X = self.X - baseline\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if hasattr(self, \"y\"):\n",
    "            return self.X[i], self.img_features[i], self.y[i], self.subject_idxs[i]\n",
    "        else:\n",
    "            return self.X[i], self.subject_idxs[i]\n",
    "        \n",
    "    @property\n",
    "    def num_channels(self) -> int:\n",
    "        return self.X.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def seq_len(self) -> int:\n",
    "        return self.X.shape[2]\n",
    "\n",
    "loader_args = {\"batch_size\": 128, \"num_workers\": 0}\n",
    "data_dir = \"data\"\n",
    "train_set = ThingsMEGDataset(\"train\", data_dir)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, **loader_args)\n",
    "val_set = ThingsMEGDataset(\"val\", data_dir)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, shuffle=False, **loader_args)\n",
    "test_set = ThingsMEGDataset(\"test\", data_dir)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, shuffle=False, **loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "max_val_acc = 0\n",
    "accuracy = Accuracy(\n",
    "    task=\"multiclass\", num_classes=train_set.num_classes, top_k=10\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def accuracy_contrastive(logits, labels):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    correct = torch.sum(preds == labels).item()\n",
    "    accuracy = correct / labels.size(0)\n",
    "    return accuracy\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    train_loss, train_acc = [], []\n",
    "\n",
    "    for X, image_features, y, subject_idxs in tqdm(dataloader, desc=\"Train\"):\n",
    "        X, image_features, subject_idxs = X.to(device), image_features.to(device), subject_idxs.to(device)\n",
    "        X = X.unsqueeze(1)\n",
    "\n",
    "        logits = model(X, image_features, subject_idxs)\n",
    "        labels = torch.arange(logits.shape[0]).to(device)\n",
    "\n",
    "        loss_e = F.cross_entropy(logits, labels)\n",
    "        loss_i = F.cross_entropy(logits.t(), labels)\n",
    "        loss = (loss_e + loss_i) / 2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = accuracy_contrastive(logits, labels)\n",
    "        train_acc.append(acc)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(train_loss), np.mean(train_acc)\n",
    "\n",
    "def validate_epoch(model, dataloader):\n",
    "    model.eval()\n",
    "    val_loss, val_acc = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, image_features, y, subject_idxs in tqdm(dataloader, desc=\"Validation\"):\n",
    "            X, image_features, subject_idxs = X.to(device), image_features.to(device), subject_idxs.to(device)\n",
    "            X = X.unsqueeze(1)\n",
    "\n",
    "            logits = model(X, image_features, subject_idxs)\n",
    "            labels = torch.arange(logits.shape[0]).to(device)\n",
    "\n",
    "            loss_e = F.cross_entropy(logits, labels)\n",
    "            loss_i = F.cross_entropy(logits.t(), labels)\n",
    "            loss = (loss_e + loss_i) / 2\n",
    "\n",
    "            acc = accuracy_contrastive(logits, labels)\n",
    "            val_acc.append(acc)\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss), np.mean(val_acc)\n",
    "\n",
    "def run_training(model, train_loader, val_loader, optimizer, epochs, save_path):\n",
    "    max_val_acc = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer)\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | train loss: {train_loss:.3f} | train acc: {train_acc:.3f} | val loss: {val_loss:.3f} | val acc: {val_acc:.3f}\")\n",
    "\n",
    "        if val_acc > max_val_acc:\n",
    "            cprint(\"New best.\", \"cyan\")\n",
    "            max_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "\n",
    "emb_dim = 512\n",
    "emb_meg = 768\n",
    "meg_encoder = MEGEncoder(out_dim=emb_meg).to(device)\n",
    "proj_meg = ProjMeg(emb_dim=emb_meg, out_dim=emb_meg).to(device)\n",
    "proj_img = ProjImg(emb_dim=emb_dim, out_dim=emb_meg).to(device)\n",
    "model = ContrastiveModel(meg_encoder, proj_meg, proj_img).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "run_training(model, train_loader, val_loader, optimizer, epochs=20, save_path=\"best_attn.pth\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "run_training(model, train_loader, val_loader, optimizer, epochs=3, save_path=\"best_attn_adamw.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 識別器の学習\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classifier(nn.Sequential):\n",
    "    def __init__(self, emb_dim=512, out_dim=512, drop=0.3):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_dim, out_dim),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(out_dim, out_dim),\n",
    "                nn.Dropout(drop),\n",
    "            )),\n",
    "            nn.LayerNorm(out_dim),\n",
    "        )\n",
    "        \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def accuracy_contrastive(logits, labels):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    correct = torch.sum(preds == labels).item()\n",
    "    accuracy = correct / labels.size(0)\n",
    "    return accuracy\n",
    "\n",
    "def train_one_epoch(model, classifier, optimizer, data_loader, epoch, epochs, is_train=True):\n",
    "    if is_train:\n",
    "        model.train()\n",
    "        classifier.train()\n",
    "        desc = \"Train\"\n",
    "    else:\n",
    "        model.eval()\n",
    "        classifier.eval()\n",
    "        desc = \"Validation\"\n",
    "    \n",
    "    total_loss, total_acc = 0, 0\n",
    "    \n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        for X, _, y, subject_idxs in tqdm(data_loader, desc=f\"{desc} Epoch {epoch+1}/{epochs}\"):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            subject_idxs = subject_idxs.to(device)\n",
    "            X = X.unsqueeze(1) \n",
    "\n",
    "            features = model.meg_encoder(X, subject_idxs)\n",
    "            features = model.proj_meg(features)\n",
    "            features = features / features.norm(dim=1, keepdim=True)\n",
    "\n",
    "            logits = classifier(features)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            acc = accuracy_multi(logits, y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "    \n",
    "    total_loss /= len(data_loader)\n",
    "    total_acc /= len(data_loader)\n",
    "    \n",
    "    return total_loss, total_acc\n",
    "\n",
    "def save_model_state(model, classifier, file_name):\n",
    "    torch.save({\n",
    "        'meg_encoder': model.meg_encoder.state_dict(),\n",
    "        'proj_meg': model.proj_meg.state_dict(),\n",
    "        'classifier': classifier.state_dict()\n",
    "    }, file_name)\n",
    "\n",
    "def load_model_state(model, classifier, file_name):\n",
    "    checkpoint = torch.load(file_name)\n",
    "    model.meg_encoder.load_state_dict(checkpoint['meg_encoder'])\n",
    "    model.proj_meg.load_state_dict(checkpoint['proj_meg'])\n",
    "    classifier.load_state_dict(checkpoint['classifier'])\n",
    "\n",
    "\n",
    "emb_dim = 512\n",
    "emb_meg = 768\n",
    "\n",
    "meg_encoder_pred = MEGEncoder(out_dim=emb_meg).to(device)\n",
    "proj_meg_pred = ProjMeg(emb_dim=emb_meg, out_dim=emb_meg).to(device)\n",
    "proj_img_pred = ProjImg(emb_dim=512, out_dim=emb_meg).to(device)\n",
    "contrastive_model_pred = ContrastiveModel(meg_encoder_pred, proj_meg_pred, proj_img_pred).to(device)\n",
    "contrastive_model_pred.load_state_dict(torch.load('best_attn_adamw_07180006.pth'))\n",
    "\n",
    "classifier = Classifier(emb_dim=emb_meg, out_dim=test_set.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(list(meg_encoder_pred.parameters()) + list(proj_meg_pred.parameters()) + list(classifier.parameters()), lr=1e-3)\n",
    "\n",
    "max_val_acc = 0\n",
    "accuracy_multi = Accuracy(task=\"multiclass\", num_classes=train_set.num_classes, top_k=10).to(device)\n",
    "\n",
    "epochs = 15\n",
    "best_val_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_one_epoch(contrastive_model_pred, classifier, optimizer, train_loader, epoch, epochs, is_train=True)\n",
    "    val_loss, val_acc = train_one_epoch(contrastive_model_pred, classifier, optimizer, val_loader, epoch, epochs, is_train=False)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | train loss: {train_loss:.3f} | train acc: {train_acc:.3f} | val loss: {val_loss:.3f} | val acc: {val_acc:.3f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        print(\"New best model saved.\")\n",
    "        best_val_acc = val_acc\n",
    "        save_model_state(contrastive_model_pred, classifier, 'best_ft_model_attn.pth')\n",
    "\n",
    "load_model_state(contrastive_model_pred, classifier, 'best_ft_model_attn_07180020.pth')\n",
    "\n",
    "optimizer = torch.optim.AdamW(list(meg_encoder_pred.parameters()) + list(proj_meg_pred.parameters()) + list(classifier.parameters()), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "epochs = 15\n",
    "best_val_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_one_epoch(contrastive_model_pred, classifier, optimizer, train_loader, epoch, epochs, is_train=True)\n",
    "    val_loss, val_acc = train_one_epoch(contrastive_model_pred, classifier, optimizer, val_loader, epoch, epochs, is_train=False)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | train loss: {train_loss:.3f} | train acc: {train_acc:.3f} | val loss: {val_loss:.3f} | val acc: {val_acc:.3f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        print(\"New best model saved.\")\n",
    "        best_val_acc = val_acc\n",
    "        save_model_state(contrastive_model_pred, classifier, 'best_ft_model_attn_adamw.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## submissionの生成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emb_dim = 512\n",
    "emb_meg = 768\n",
    "meg_encoder_pred = MEGEncoder(out_dim=emb_meg).to(device)\n",
    "proj_meg_pred = ProjMeg(emb_dim=emb_meg, out_dim=emb_meg).to(device)\n",
    "classifier = Classifier(emb_dim=emb_meg, out_dim=test_set.num_classes).to(device)\n",
    "\n",
    "checkpoint = torch.load('best_ft_model_emb_adamw.pth')\n",
    "\n",
    "meg_encoder_pred.load_state_dict(checkpoint['meg_encoder'])\n",
    "proj_meg_pred.load_state_dict(checkpoint['proj_meg'])\n",
    "classifier.load_state_dict(checkpoint['classifier'])\n",
    "\n",
    "preds = [] \n",
    "meg_encoder_pred.eval()\n",
    "proj_meg_pred.eval()\n",
    "classifier.eval()\n",
    "for X, subject_idxs in tqdm(test_loader, desc=\"Validation\"):        \n",
    "    X = X.to(device)\n",
    "    subject_idxs = subject_idxs.to(device)\n",
    "    X = X.unsqueeze(1) \n",
    "\n",
    "    features = meg_encoder_pred(X, subject_idxs)\n",
    "    features = proj_meg_pred(features)\n",
    "    features = features / features.norm(dim=1, keepdim=True)\n",
    "\n",
    "    logits = classifier(features)\n",
    "\n",
    "    preds.append(logits.detach().cpu())\n",
    "    \n",
    "preds = torch.cat(preds, dim=0).numpy()\n",
    "np.save(os.path.join(\"data\", \"submission\"), preds)\n",
    "cprint(f\"Submission {preds.shape} saved at data\", \"cyan\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
